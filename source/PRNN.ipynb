{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4100558e2079f81c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Probabilistic Recurrent Neural Networks\n",
    "\n",
    "Consider a set of words in a text. We have a system whose internal state is of dimension L, \n",
    "meaning it can store the information of L words. The system goes through the words 1 by one, \n",
    "and we want to store the information of the words in the system.\n",
    "\n",
    "The objective is to have the information of any word in the text decays with a power law of the distance \n",
    "between the word and the current word. This would address the problem of exponential decay of informationin RNNs.\n",
    "\n",
    "The objective will be achieved by first applying a contextual embedding of the words, which\n",
    "will store the information of the given word and the D words around it. \n",
    "\n",
    "Then, the system will store each word it encounters in the memory, that can contain up to L words.\n",
    "At each step, the system will update the memory by adding the information of the current word,\n",
    "and removing the information of the oldest words in the memory with a probability depending on\n",
    "how many steps ago a word was added to the memory:\n",
    "\n",
    "$$ p = 1 - \\left(\\frac{n-1}{n}\\right)^\\alpha $$\n",
    "\n",
    "where n is the number of words that passed since the word was added to the memory, and alpha \n",
    "is the exponent of the power law.\n",
    "\n",
    "The probabilistic routine we just discussed can in principle be substituted by a neural network, that\n",
    "would be able to learn the optimal way to update the memory as a function of the new input and the\n",
    "current state of the memory. This will be the next step of the project.\n",
    "\n",
    "The expect value of the quantity of information relating to a single word is obviously a fraction of the original information that can be expressed as:\n",
    "\n",
    "$$ p = \\frac{c}{n^\\alpha}$$\n",
    "\n",
    "by construction, where $ c = \\sum_{i=1}^{\\infty}\\left(1/n\\right)^\\alpha $.\n",
    "\n",
    "On the other hand, the $\\sigma$ of the distribution is dependent on the number of neighbours used in the contextual embedding. The determination of such relation is important to ensure that information can in fact be reliably stored in the internal state, and we will proceed in deriving it in the first part of the project\n",
    "\n",
    "We can start to understand the permanence of information in the memory by using as text the one-hot vectors, embedd them with the contextual embedding, and see how the sum of the scalar products of the memory vectors score with a given word vector:\n",
    "\n",
    "$$ I_i = \\sum_{j=0}^{L} v_j \\cdot w_i $$\n",
    "\n",
    "where w_i is the one hot encoded vector with 1 in position i and 0 elsewhere, and $v_j$ are the vectors of the memory at a given time\n",
    "\n",
    "The behaviour of the $I_i$ will be power law with respeect to the index i.\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "The information relating to a single word, is not only given by its contextual embedding, that in our case is simply a weighted average of the neighbouring words. \n",
    "\n",
    "Thus, a positional encoding is added to make sure the information of the position is not only on the probabilities of forgetting vectors, but in the vectors themselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ad1feec676ef1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:00:01.330099Z",
     "start_time": "2023-12-22T11:00:01.328162Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from memory import Memory\n",
    "from text import Text\n",
    "from model import Model\n",
    "import config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
